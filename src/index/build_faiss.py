import json
import pickle
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from src.config import CHUNKS_PATH, FAISS_INDEX_PATH, FAISS_META_PATH, MODEL_NAME


def build_index():
    model = SentenceTransformer(MODEL_NAME)

    texts = []
    metas = []

    with open(CHUNKS_PATH, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            try:
                obj = json.loads(line)
            except json.JSONDecodeError:
                print("‚ö† –ü—Ä–æ–ø—É—â–µ–Ω–∞ –ø–æ–≤—Ä–µ–∂–¥—ë–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ chunks.jsonl")
                continue

            texts.append(obj["text"])
            metas.append({"id": obj["id"], "page": obj["page"]})

    if not texts:
        print("‚ö† –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
        return

    embeddings = model.encode(texts, normalize_embeddings=True)
    embeddings = np.array(embeddings, dtype="float32")

    index = faiss.IndexFlatIP(embeddings.shape[1])
    index.add(embeddings)

    faiss.write_index(index, str(FAISS_INDEX_PATH))

    with open(FAISS_META_PATH, "wb") as f:
        pickle.dump({"texts": texts, "metas": metas}, f)

    print(f"üî• –ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω. –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(texts)}")
